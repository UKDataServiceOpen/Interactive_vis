{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo=TRUE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide\n",
    "\n",
    "In this notebook we will create an interactive map of the UK which displays the % of Trans men in each local authority.\n",
    "There are 331 local authorities in the UK, and we are using data collected in the 2021 UK Census which included 2 new questions on sexuality and gender identity.\n",
    "The following data used are:\n",
    "\n",
    "-   [Gender identity (detailed)](https://www.ons.gov.uk/datasets/TS070/editions/2021/versions/3) - this dataset classifies usual residents aged 16 years and over in England and Wales by gender identity.\n",
    "-   [Local Authority District Boundaries](https://geoportal.statistics.gov.uk/datasets/bb53f91cce9e4fd6b661dc0a6c734a3f_0/about) - this file contains the digital vector boundaries for Local Authority Districts in the UK as of May 2022.\n",
    "\n",
    "## Install packages\n",
    "\n",
    "If you're running this code on your own PC (and not through the Binder link) then you're going to want to uncomment the lines below so you can install the requisite packages. Another thing to remember is to set your working directory to the correct folder. Otherwise reading in data will be difficult. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    " # install.packages(\"leaflet\")\n",
    " # install.packages(\"sf\")\n",
    " # install.packages(\"dplyr\")\n",
    " # install.packages(\"readr\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# used to read-in datasets\n",
    "library(readr)\n",
    "# used to manipulate datasets\n",
    "library(dplyr)\n",
    "# used to read-in spatial data, shapefiles\n",
    "library(sf)\n",
    "# used to create interactive maps\n",
    "library(leaflet)\n",
    "# used to scrape data from websites\n",
    "library(httr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-in dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# First, let's read in our gender identity dataset\n",
    "\n",
    "df <- read_csv('../Data/GI_det.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Use head function to check out the first few rows - but can also access df via environment pane\n",
    "\n",
    "head(df, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Before we can calculate the %'s of trans men in each local authority, it's good to do some housekeeping and get our dataframe in order.\n",
    "\n",
    "There's a few things that need sorting including:\n",
    "\n",
    "1.  renaming columns so they are easier to reference\n",
    "2.  removing 'Does not apply' from gender identity category\n",
    "\n",
    "\n",
    "### Pipe operator - %\\>%\n",
    "\n",
    "The pipe operator is used to pass the result of one function directly into the next one.\n",
    "E.g. let's say we had some code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "sorted_data <- my_data %\\>% filter(condition) %\\>% arrange(sorting_variable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're doing is using the pipe operator to pass my_data to the filter() function, and the result of this is then passed to the arrange() function.\n",
    "\n",
    "Basically, pipes allow us to chain together a sequence of functions in a way that's easy to read and understand.\n",
    "\n",
    "In the code below we use the pipe operator to pass our dataframe to the rename function.\n",
    "\n",
    "This basically supplies the rename function with its first argument, which is the dataframe to filter on.\n",
    "\n",
    "### 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Rename columns using the rename function from dplyr\n",
    "# Specify what you want to rename the column to, and supply the original column string\n",
    "\n",
    "df <- df %>% \n",
    "  rename(LA_code = `Lower tier local authorities Code`,\n",
    "         # backticks ` necessary when names are syntactically invalid, e.g. spaces, special characters etc.\n",
    "         LA_name = `Lower tier local authorities`,\n",
    "         GI_code = `Gender identity (8 categories) Code`,\n",
    "         GI_cat = `Gender identity (8 categories)`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Let's use the colnames function to see if it worked\n",
    "\n",
    "colnames(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "### Logical operators - ==, !=, \\<, \\>, \\<=, \\>=, &, \\|, !\n",
    "\n",
    "Logical operators are used to perform comparisons between values or expressions, which result in a logical (Boolean) value of 'TRUE' or 'FALSE'.\n",
    "\n",
    "In the code below we use the '!=' 'Does not equal' operator which tests if the GI_cat value in each row of the df does not equal the string 'Does not apply'.\n",
    "\n",
    "For each row where GI_cat is not equal to 'Does not apply', the expression valuates to TRUE.\n",
    "\n",
    "We filter so we only keep rows where this expression evaluates to TRUE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Use dplyr's filter function to get rid of 'Does not apply'\n",
    "# Use '!=' to keep everything except 'Does not apply' category\n",
    "\n",
    "df <- df %>% filter(GI_cat != 'Does not apply')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dollar sign operator - $\n",
    "\n",
    "This operator is used to access elements, such as columns of a dataframe, by name.Below, we use it to access the gender identity category column, where we want to view the unique values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Unique function can be applied to a column in a df to see which values are in that column\n",
    "# Let's see if 'Does not apply' has been successfully dropped\n",
    "\n",
    "unique(df$GI_cat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "Now onto the more interesting stuff.\n",
    "The data pre-processing stage involves preparing and transforming data into a suitable format for further analysis.It can involve selecting features, transforming variables, and creating new variables.For our purposes, we need to create a new column 'Percentages' which contains the % of Trans men in each local authority. \n",
    "\n",
    "So, we'll need to first calculate the % of each gender identity category for each local authority. Then, we'll want to filter our dataset so that we only keep the responses related to Trans men.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Use group_by to group the dataframe by the LA_name column\n",
    "# Use mutate to perform calculation within each LA_name group, convert result to a % by multiplying by 100\n",
    "# round() is used to round %'s to 2 decimal places\n",
    "\n",
    "df <- df %>%\n",
    "  group_by(LA_name) %>%\n",
    "  mutate(Percentage = round(Observation / sum(Observation) * 100, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Let's check out the results\n",
    "\n",
    "head(df, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Use filter() to only keep rows where GI_cat equals 'Trans man'\n",
    "df <- df %>% \n",
    "  filter(GI_cat == 'Trans man') %>%\n",
    "  # Use select() with '-' to remove 'Observation' column\n",
    "  select(-Observation) %>% \n",
    "  # Use distinct() to remove duplicate rows, as a precaution\n",
    "  distinct() %>% \n",
    "  # Use ungroup() to remove grouping - resetting the dataframes state after performing group operations is good practice\n",
    "  ungroup()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the results\n",
    "head(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-in shapefile\n",
    "\n",
    "Now that we have our gender identity dataset sorted, we can start on the mapping process. And that starts with reading in our shapefile, which we should have downloaded from the geoportal. If (like me) you don't work with spatial data much, you might assume that you only need the shapefile, and you might delete the others that come with the folder. However, a shapefile is not just a single .shp file, but a collection of files that work together, and each of these files plays a crucial role in defining the shapefile's data and behaviour. When you try and read a shapefile into R, the software expects all components to be present, and missing them can lead to errors or incorrect spatial references. E.g. without the .dbf file, you'd lose all attribute data associated with the geographic features, and without the .shx file you might not be able to read the .shp file altogether. \n",
    "\n",
    "**TLDR: Make sure when you download the shapefile folder you keep all the files!**\n",
    "\n",
    "Anyway, let's get started.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Download shapefiles from geoportal \n",
    "\n",
    "# URL for the direct download of the shapefile\n",
    "url <- \"https://services1.arcgis.com/ESMARspQHYMw9BZ9/arcgis/rest/services/Local_Authority_Districts_May_2022_UK_BFE_V3_2022/FeatureServer/replicafilescache/Local_Authority_Districts_May_2022_UK_BFE_V3_2022_3331011932393166417.zip\"\n",
    "\n",
    "# Create a temporary directory\n",
    "tmp_dir <- tempdir()\n",
    "print(paste(\"Created temporary directory:\", tmp_dir))\n",
    "\n",
    "# Set destination file path\n",
    "dest_file <- file.path(tmp_dir, \"shapefile.zip\")\n",
    "\n",
    "# Download the shapefile\n",
    "response <- GET(url, write_disk(dest_file, overwrite = TRUE))\n",
    "\n",
    "# Check if the download was successful\n",
    "if (response$status_code == 200)\n",
    "  print(\"Download successful\")\n",
    "  \n",
    "  # Unzip the file within the temporary directory\n",
    "  unzip(dest_file, exdir = tmp_dir)\n",
    "  print(paste(\"Files extracted to:\", tmp_dir))\n",
    "  \n",
    "  # List all files in the temporary directory to verify extraction\n",
    "  extracted_files <- list.files(tmp_dir)\n",
    "  print(\"Extracted files:\")\n",
    "  print(extracted_files)\n",
    "  \n",
    "  # Define the path to the actual shapefile (.shp)\n",
    "  shapefile_path <- file.path(tmp_dir, 'LAD_MAY_2022_UK_BFE_V3.shp')\n",
    "  \n",
    "  # Read in shapefile to a simple features object\n",
    "  # st_read() reads in spatial data to a 'simple features' object\n",
    "  sf <- st_read(shapefile_path)\n",
    "  print(\"Shapefile loaded successfully.\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Let's check it out \n",
    "head(sf)\n",
    "# Better to just view via environment pane\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect dimensions\n",
    "dim(sf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# length() with the unique() function gives us the number of unique values in a column\n",
    "\n",
    "length(unique(sf$LAD22NM))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning shapefile\n",
    "\n",
    "Hmm.We have 331 local authorities in our dataset that we want to plot, but there are 374 listed here.\n",
    "We'll need to remove the local authorities that don't match the ones in our df.\n",
    "\n",
    "1. rename columns to match 'df'\n",
    "2. get rid of redundant Local Authorities\n",
    "\n",
    "### 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Use rename function so sf columns match those in original df\n",
    "\n",
    "sf <- sf %>% \n",
    "  rename(LA_code = LAD22CD, \n",
    "         LA_name = LAD22NM)\n",
    "\n",
    "# Let's see if it worked\n",
    "colnames(sf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Replace specific values in the LA_name column using recode()\n",
    "\n",
    "sf$LA_name <- sf$LA_name %>% \n",
    "  recode(`Bristol, City of` = \"Bristol\", \n",
    "         `Kingston upon Hull, City of` = \"Kingston upon Hull\", \n",
    "         `Herefordshire, County of` = \"Herefordshire\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "### %in% operator\n",
    "\n",
    "This is used to check if elements of one list are in another list.\n",
    "Much like the logical operators, it returns a boolean value TRUE or FALSE.\n",
    "And we only keep rows in the LA_code for the 'sf' dataset, if they are present in the LA_code column in 'df'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Use filter() with %in% and unique() to only keep LA's that match \n",
    "\n",
    "sf <- sf %>% \n",
    "  filter(LA_code %in% unique(df$LA_code))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see how it looks.. \n",
    "# We should have 331 unique LA_codes\n",
    "length(unique(sf$LA_code))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing shapefile\n",
    "\n",
    "When it comes to mapping our data, it is important that we know which Coordinate Reference System (CRS) we are working with. Simply put, the CRS is a way to describe how the spatial data in the 'sf' object maps to locations on earth. The CRS is just a way of translating 3D reality into 2D maps. And when it comes to using mapping libraries like 'leaflet', knowing the CRS is important because leaflet expects coordinates in a specific format (usually latitude and longitude), which is EPSG:4326. If our CRS isn't in this format then we might need to transform it so that it matches what leaflet expects. Let's go ahead and see what our CRS is saying. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# st_crs() shows our CRS info\n",
    "st_crs(sf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# To transform our crs to EPSG: 4326, simply use st_transform() and specify the crs\n",
    "# Note: you don't have to use the %>% pipe operator all the time\n",
    "sf <- st_transform(sf, crs = 4326)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge datasets\n",
    "\n",
    "What we want to do now is merge our 'df' dataframe with our 'sf' spatial object, so that we can directly access the data and map it!\n",
    "\n",
    "When you use the merge function in R, the order in which you place the data matters in terms of the result's class type and spatial attributes. \n",
    "So, in terms of class type, we have a dataframe and a spatial object. By placing 'sf' first, the result will be a spatial object, which is important because this retains the spatial characteristics and geometry columns of the 'sf' object. We merge the columns on the LA_code and LA_name columns which are present in both datasets. \n",
    "\n",
    "### 'c' function\n",
    "\n",
    "Don't overthink it. It's just a way to group items together in R, whether for defining a set of values to work with, specifying parameters for a function, or any number of other uses where a list of items is needed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Merge the dataframes\n",
    "merged <- merge(sf, df, by = c('LA_code', 'LA_name'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Let's check it out\n",
    "head(merged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "## Building our interactive map\n",
    "\n",
    "Finally, we can now build out interactive map using leaflet. You can see from the 'geometry' column that we're working with 'MULTIPOLYGON's' and 'POLYGON's'. Multipolygons are a collection of polygons grouped together as a single geometric entity. Basically, multipolygons are good at representing complex shapes. We also have some standard polygons too. In total we have 331 shapes to plot, each representing a local authority. You can take a look at these separate shapes by using the plot function and indexing the row and column (see below). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot(sf[1, 'geometry'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below has helpful code comments that should help you grasp what each bit of the code is doing. But, to provide the overall picture, what we have below is some code for our colour palette which will create a colour scale for the range of values in our 'Percentage' column. Then, we create our interactive map which we've named 'uk_map'. We center our map, add some default map tiles, add our polygons, colour them, then add in the interactive elements such as highlight options (how background changes when cursor hovers over a shape) and label (which specifies tooltips). Then, we add a legend. Finally, we can display this interactive map. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Define the color palette for filling in our multipolygon shapes\n",
    "# domain sets the range of data values that the colour scale should cover\n",
    "color_palette <- colorNumeric(palette = \"YlGnBu\", domain = merged$Percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Use leaflet function with 'merged' dataset\n",
    "uk_map <- leaflet(merged) %>%\n",
    "  # Centers the map on long and lat for UK\n",
    "  setView(lng = -3.0, lat = 53, zoom = 6) %>%\n",
    "  # Adds default map tiles (the visual image of the map)\n",
    "  addTiles() %>%\n",
    "  # Adds multipolygons to the map, and colours them based on the 'Percentage' column\n",
    "  # We use the palette we created above\n",
    "  addPolygons(\n",
    "    fillColor = ~color_palette(Percentage),\n",
    "    weight = 1, # Set the border weight to 1 for thinner borders\n",
    "    color = \"#000000\",\n",
    "    fillOpacity = 0.7,\n",
    "    highlightOptions = highlightOptions(color = \"white\", weight = 2, bringToFront = TRUE),\n",
    "    label = ~paste(LA_name, \":\", Percentage, \"%\"), # This will create tooltips showing the info\n",
    "    labelOptions = labelOptions(\n",
    "      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n",
    "      textsize = \"12px\", direction = \"auto\") # Adjust text size as needed\n",
    "  ) %>%\n",
    "  addLegend(pal = color_palette, values = ~Percentage, opacity = 0.7, title = \"Percentage\", position = \"topright\")\n",
    "\n",
    "# Render the map\n",
    "uk_map\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
